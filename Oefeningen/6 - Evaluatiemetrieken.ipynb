{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-26T21:33:04.031886Z",
     "start_time": "2024-08-26T21:33:04.021689Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "\n",
    "# Functies om de metriek te berekenen voor een binaire classificatie\n",
    "\n",
    "def true_positives(cm):\n",
    "    return cm[0, 0]\n",
    "\n",
    "def true_negatives(cm):\n",
    "    return cm[1, 1]\n",
    "\n",
    "def false_positives(cm):\n",
    "    return cm[1, 0]\n",
    "\n",
    "def false_negatives(cm):\n",
    "    return cm[0, 1]\n",
    "\n",
    "def precision(cm):\n",
    "    TP = true_positives(cm)\n",
    "    FP = false_positives(cm)\n",
    "    return TP / (TP + FP)\n",
    "\n",
    "def recall(cm):\n",
    "    TP = true_positives(cm)\n",
    "    FN = false_negatives(cm)\n",
    "    return TP / (TP + FN)\n",
    "\n",
    "def accuracy(cm):\n",
    "    TP = true_positives(cm)\n",
    "    TN = true_negatives(cm)\n",
    "    FP = false_positives(cm)\n",
    "    FN = false_negatives(cm)\n",
    "    return (TP + TN) / (TP + TN + FP + FN)\n",
    "\n",
    "def f1_score(cm):\n",
    "    prec = precision(cm)\n",
    "    rec = recall(cm)\n",
    "    return 2 * (prec * rec) / (prec + rec)\n",
    "\n",
    "# Functies om de metriek te berekenen voor multiclass classificatie\n",
    "\n",
    "def true_positives_multiclass(cm, class_idx):\n",
    "    return cm[class_idx, class_idx]\n",
    "\n",
    "def false_positives_multiclass(cm, class_idx):\n",
    "    return np.sum(cm[:, class_idx]) - cm[class_idx, class_idx]\n",
    "\n",
    "def false_negatives_multiclass(cm, class_idx):\n",
    "    return np.sum(cm[class_idx, :]) - cm[class_idx, class_idx]\n",
    "\n",
    "def true_negatives_multiclass(cm, class_idx):\n",
    "    return np.sum(cm) - (true_positives_multiclass(cm, class_idx) + false_positives_multiclass(cm, class_idx) + false_negatives_multiclass(cm, class_idx))\n",
    "\n",
    "def precision_multiclass(cm, class_idx):\n",
    "    TP = true_positives_multiclass(cm, class_idx)\n",
    "    FP = false_positives_multiclass(cm, class_idx)\n",
    "    return TP / (TP + FP)\n",
    "\n",
    "def recall_multiclass(cm, class_idx):\n",
    "    TP = true_positives_multiclass(cm, class_idx)\n",
    "    FN = false_negatives_multiclass(cm, class_idx)\n",
    "    return TP / (TP + FN)\n",
    "\n",
    "def accuracy_multiclass(cm, class_idx):\n",
    "    TP = true_positives_multiclass(cm, class_idx)\n",
    "    TN = true_negatives_multiclass(cm, class_idx)\n",
    "    FP = false_positives_multiclass(cm, class_idx)\n",
    "    FN = false_negatives_multiclass(cm, class_idx)\n",
    "    return (TP + TN) / (TP + TN + FP + FN)\n",
    "\n",
    "### Uitleg van de code:\n",
    "#- De functies **true_positives**, **true_negatives**, **false_positives**, en **false_negatives** worden gebruikt om respectievelijk TP, TN, FP, en FN te berekenen voor binaire classificatie.\n",
    "#- Voor multiclass classificatie zijn er aangepaste functies zoals **true_positives_multiclass** enz., waarbij de klasse-index wordt gebruikt om de waarden per klasse te berekenen.\n",
    "#- De functies **precision**, **recall**, **accuracy**, en **f1_score** worden gebruikt om deze metrieken te berekenen op basis van de waarden van TP, TN, FP, en FN.\n",
    "#\n",
    "#### Gebruik:\n",
    "#- **confusion_matrix_binary**: Gebruik deze om metrieken te berekenen voor een binaire classifier.\n",
    "#- **confusion_matrix_multiclass**: Gebruik deze om metrieken te berekenen voor een multiclass classifier.\n",
    "#\n",
    "#Sla deze code op in een bestand en voer het uit om de gevraagde metrieken te berekenen voor de gegeven confusion matrices."
   ],
   "id": "285573b5956bc58b",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# oefeningen evaluatiemetrieken\n",
    "\n",
    "## vraag 1\n",
    "### a) Wat zijn de waarden voor TP, TN, FP, FN?\n",
    "\n",
    "- **True Positives (TP)**: Dit zijn de gevallen waarin het model correct voorspelt dat de uitkomst \"YES\" is.\n",
    "  - TP = 100\n",
    "\n",
    "- **True Negatives (TN)**: Dit zijn de gevallen waarin het model correct voorspelt dat de uitkomst \"NO\" is.\n",
    "  - TN = 50\n",
    "\n",
    "- **False Positives (FP)**: Dit zijn de gevallen waarin het model voorspelt dat de uitkomst \"YES\" is, terwijl de werkelijke uitkomst \"NO\" is.\n",
    "  - FP = 10\n",
    "\n",
    "- **False Negatives (FN)**: Dit zijn de gevallen waarin het model voorspelt dat de uitkomst \"NO\" is, terwijl de werkelijke uitkomst \"YES\" is.\n",
    "  - FN = 5\n",
    "\n",
    "### b) Bereken nu de accuracy, precision en recall.\n",
    "\n",
    "De formules voor deze metriek zijn als volgt:\n",
    "\n",
    "- **Accuracy**: \\(\\text{Accuracy} = \\frac{TP + TN}{TP + TN + FP + FN}\\)\n",
    "- **Precision**: \\(\\text{Precision} = \\frac{TP}{TP + FP}\\)\n",
    "- **Recall**: \\(\\text{Recall} = \\frac{TP}{TP + FN}\\)\n",
    "\n",
    "We berekenen:\n",
    "\n",
    "- **Accuracy**: \\(\\frac{100 + 50}{100 + 50 + 10 + 5} = \\frac{150}{165} \\approx 0.9091\\) (90.91%)\n",
    "- **Precision**: \\(\\frac{100}{100 + 10} = \\frac{100}{110} \\approx 0.9091\\) (90.91%)\n",
    "- **Recall**: \\(\\frac{100}{100 + 5} = \\frac{100}{105} \\approx 0.9524\\) (95.24%)\n",
    "\n",
    "### c) Bereken de F₁- en F₁.₅-measures.\n",
    "\n",
    "De F₁-measure en F₁.₅-measure kunnen als volgt worden berekend:\n",
    "\n",
    "- **F₁-score**: \\(\\text{F}_1 = 2 \\times \\frac{\\text{Precision} \\times \\text{Recall}}{\\text{Precision} + \\text{Recall}}\\)\n",
    "- **F₁.₅-score**: \\(\\text{F}_{1.5} = (1 + 1.5^2) \\times \\frac{\\text{Precision} \\times \\text{Recall}}{1.5^2 \\times \\text{Precision} + \\text{Recall}}\\)\n",
    "\n",
    "We berekenen:\n",
    "\n",
    "- **F₁-score**: \\(2 \\times \\frac{0.9091 \\times 0.9524}{0.9091 + 0.9524} \\approx 0.9302\\) (93.02%)\n",
    "- **F₁.₅-score**: \\( (1 + 1.5^2) \\times \\frac{0.9091 \\times 0.9524}{1.5^2 \\times 0.9091 + 0.9524} \\approx 0.9449\\) (94.49%)\n",
    "\n",
    "### d) Wat is de TPR en FPR? Vergelijk met je recall. Wat stel je vast?\n",
    "\n",
    "- **True Positive Rate (TPR) / Recall**: TPR is hetzelfde als de Recall.\n",
    "  - TPR = 0.9524 (95.24%)\n",
    "\n",
    "- **False Positive Rate (FPR)**: \\(\\text{FPR} = \\frac{FP}{FP + TN}\\)\n",
    "  - FPR = \\(\\frac{10}{10 + 50} = \\frac{10}{60} \\approx 0.1667\\) (16.67%)\n"
   ],
   "id": "1395208700b0a6e5"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-26T21:33:30.442477Z",
     "start_time": "2024-08-26T21:33:30.437012Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#e) Maak van bovenstaande tabel een (confusion) matrix in Python.\n",
    "import numpy as np\n",
    "\n",
    "# Definieer de confusion matrix\n",
    "confusion_matrix = np.array([[100, 5], [10, 50]])\n",
    "print(confusion_matrix)"
   ],
   "id": "db9c035d5f46c9a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[100   5]\n",
      " [ 10  50]]\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-26T21:33:32.558976Z",
     "start_time": "2024-08-26T21:33:32.553083Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# f) Schrijf een aparte functie voor elk metriek en controleer je oplossing.\n",
    "# Definieer de confusion matrix\n",
    "confusion_matrix = np.array([[100, 5], [10, 50]])\n",
    "\n",
    "# Bereken de metriek\n",
    "print(\"Accuracy:\", accuracy(confusion_matrix))\n",
    "print(\"Precision:\", precision(confusion_matrix))\n",
    "print(\"Recall:\", recall(confusion_matrix))\n",
    "print(\"F1 Score:\", f1_score(confusion_matrix))\n",
    "print(\"F1.5 Score:\", f1_5_score(confusion_matrix))\n",
    "print(\"TPR:\", tpr(confusion_matrix))\n",
    "print(\"FPR:\", fpr(confusion_matrix))"
   ],
   "id": "initial_id",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9090909090909091\n",
      "Precision: 0.9090909090909091\n",
      "Recall: 0.9523809523809523\n",
      "F1 Score: 0.9302325581395349\n",
      "F1.5 Score: 0.9386281588447652\n",
      "TPR: 0.9523809523809523\n",
      "FPR: 0.16666666666666666\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## vraag 2\n",
    "\n",
    "### a) Wat zijn de waarden voor TP, TN, FP, FN?\n",
    "\n",
    "Uit de gegeven confusion matrix kunnen we de volgende waarden afleiden voor klasse A:\n",
    "\n",
    "- **True Positives (TP)**: Dit zijn de gevallen waarin het model correct voorspelt dat de uitkomst \"A\" is.\n",
    "  - TP = 100\n",
    "\n",
    "- **True Negatives (TN)**: Dit zijn de gevallen waarin het model correct voorspelt dat de uitkomst \"B\" is.\n",
    "  - TN = 5\n",
    "\n",
    "- **False Positives (FP)**: Dit zijn de gevallen waarin het model voorspelt dat de uitkomst \"A\" is, terwijl de werkelijke uitkomst \"B\" is.\n",
    "  - FP = 50\n",
    "\n",
    "- **False Negatives (FN)**: Dit zijn de gevallen waarin het model voorspelt dat de uitkomst \"B\" is, terwijl de werkelijke uitkomst \"A\" is.\n",
    "  - FN = 0\n",
    "\n",
    "### b) Bereken met je zelfgeschreven Python-functies de accuracy, precision, recall, en F₁.\n",
    "\n",
    "Je kunt de volgende Python code gebruiken om de gevraagde metriek te berekenen:\n",
    "\n",
    "```python\n",
    "# Bereken de metriek\n",
    "print(\"Accuracy:\", accuracy(confusion_matrix))\n",
    "print(\"Precision:\", precision(confusion_matrix))\n",
    "print(\"Recall:\", recall(confusion_matrix))\n",
    "print(\"F1 Score:\", f1_score(confusion_matrix))\n",
    "```\n",
    "\n",
    "Uitvoeren van deze code geeft de volgende resultaten:\n",
    "\n",
    "- **Accuracy**: \\(\\frac{100 + 5}{100 + 50 + 0 + 5} = \\frac{105}{155} \\approx 0.6774\\) (67.74%)\n",
    "- **Precision**: \\(\\frac{100}{100 + 50} = \\frac{100}{150} \\approx 0.6667\\) (66.67%)\n",
    "- **Recall**: \\(\\frac{100}{100 + 0} = \\frac{100}{100} = 1.0\\) (100%)\n",
    "- **F₁ Score**: \\(2 \\times \\frac{0.6667 \\times 1.0}{0.6667 + 1.0} \\approx 0.8\\) (80%)\n",
    "\n",
    "### c) Is dit een goede classifier?\n",
    "\n",
    "Om te bepalen of dit een goede classifier is, moeten we de resultaten in context plaatsen:\n",
    "\n",
    "- **Recall** is perfect (100%), wat betekent dat alle echte gevallen van \"A\" correct worden geïdentificeerd. Dit is zeer belangrijk in situaties waar het niet missen van een bepaalde klasse (zoals in het geval van een ziekte) cruciaal is.\n",
    "  \n",
    "- **Precision** is echter lager (66.67%), wat betekent dat een aanzienlijk deel van de voorspellingen voor \"A\" eigenlijk \"B\" had moeten zijn.\n",
    "\n",
    "- **Accuracy** is gematigd (67.74%), wat aangeeft dat het model in het algemeen niet altijd correct is.\n",
    "\n",
    "De **F₁-score** van 0.8 (80%) is redelijk, maar de lage precisie suggereert dat het model vaak foutpositieven genereert, wat in sommige gevallen problematisch kan zijn. \n",
    "\n",
    "Kortom, deze classifier is goed in het identificeren van alle werkelijke \"A\"-gevallen (hoge recall), maar maakt te veel fouten door \"B\" ten onrechte als \"A\" te classificeren (lage precisie). Afhankelijk van de specifieke toepassing zou het model misschien moeten worden verbeterd, vooral als precisie belangrijk is."
   ],
   "id": "5d294ae2e4fa8ab3"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## vraag 3\n",
    "\n",
    "### a) Wat zijn de waarden voor TP, FP, FN voor de klasse Gras?\n",
    "\n",
    "Voor de klasse \"Gras\" kunnen we de volgende waarden berekenen uit de gegeven confusion matrix:\n",
    "\n",
    "- **True Positives (TP)** voor Gras: Dit is het aantal keren dat de klasse correct is voorspeld als \"Gras\".\n",
    "  - TP = 908\n",
    "\n",
    "- **False Positives (FP)** voor Gras: Dit zijn de gevallen waarin iets anders dan Gras onterecht is voorspeld als Gras. Dit betekent dat we moeten kijken naar de kolom \"Gras\", maar alle rijen behalve de \"Gras\" rij.\n",
    "  - FP = 0 (Asfalt) + 1 (Beton) + 0 (Boom) + 4 (Gebouw) = 5\n",
    "\n",
    "- **False Negatives (FN)** voor Gras: Dit zijn de gevallen waarin iets wat werkelijk \"Gras\" is, niet correct is voorspeld als Gras, maar als iets anders. Dit betekent dat we moeten kijken naar de rij \"Gras\", maar alle kolommen behalve de \"Gras\" kolom.\n",
    "  - FN = 0 (Asfalt) + 0 (Beton) + 0 (Boom) + 0 (Gebouw) = 0"
   ],
   "id": "2372c98b34bc34b3"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-26T21:33:59.396604Z",
     "start_time": "2024-08-26T21:33:58.909913Z"
    }
   },
   "cell_type": "code",
   "source": [
    "### b) Bereken nu de accuracy, precision en recall per klasse met deze functies.\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Definieer de confusion matrix\n",
    "confusion_matrix = np.array([\n",
    "    [2385, 0, 0, 0, 12],  # Asfalt\n",
    "    [4, 332, 1, 0, 0],    # Beton\n",
    "    [0, 0, 908, 0, 0],    # Gras\n",
    "    [1, 0, 8, 1084, 6],   # Boom\n",
    "    [4, 1, 0, 9, 2053]    # Gebouw\n",
    "])\n",
    "\n",
    "\n",
    "# Klassen indexen\n",
    "classes = ['Asfalt', 'Beton', 'Gras', 'Boom', 'Gebouw']\n",
    "\n",
    "# Bereken de metriek voor elke klasse\n",
    "for idx, klass in enumerate(classes):\n",
    "    print(f\"Klasse: {klass}\")\n",
    "    print(f\"  Precision: {precision(confusion_matrix, idx):.4f}\")\n",
    "    print(f\"  Recall: {recall(confusion_matrix, idx):.4f}\")\n",
    "    print(f\"  Accuracy: {accuracy(confusion_matrix, idx):.4f}\")\n",
    "    print()"
   ],
   "id": "6e5e9a109e6d5ddf",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Klasse: Asfalt\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "precision() takes 1 positional argument but 2 were given",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[7], line 21\u001B[0m\n\u001B[0;32m     19\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m idx, klass \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(classes):\n\u001B[0;32m     20\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mKlasse: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mklass\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m---> 21\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m  Precision: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[43mprecision\u001B[49m\u001B[43m(\u001B[49m\u001B[43mconfusion_matrix\u001B[49m\u001B[43m,\u001B[49m\u001B[38;5;250;43m \u001B[39;49m\u001B[43midx\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;132;01m:\u001B[39;00m\u001B[38;5;124m.4f\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m     22\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m  Recall: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mrecall(confusion_matrix,\u001B[38;5;250m \u001B[39midx)\u001B[38;5;132;01m:\u001B[39;00m\u001B[38;5;124m.4f\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m     23\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m  Accuracy: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00maccuracy(confusion_matrix,\u001B[38;5;250m \u001B[39midx)\u001B[38;5;132;01m:\u001B[39;00m\u001B[38;5;124m.4f\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n",
      "\u001B[1;31mTypeError\u001B[0m: precision() takes 1 positional argument but 2 were given"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### c) Is dit een goede classifier?\n",
    "\n",
    "Of een classifier goed is, hangt af van de waarden van de metriek, zoals de precision, recall, en accuracy per klasse, evenals de specifieke context en eisen van de taak. \n",
    "\n",
    "- Als de **precision** en **recall** hoog zijn (dicht bij 1) voor alle klassen, is de classifier goed.\n",
    "- Als bepaalde klassen veel lagere precision of recall hebben, betekent dit dat de classifier moeite heeft om die specifieke klassen goed te voorspellen, wat een probleem kan zijn afhankelijk van het gebruiksscenario.\n",
    "\n",
    "Door de output van de bovenstaande code te bekijken, kun je beoordelen of de classifier goed presteert voor alle klassen of dat er ruimte is voor verbetering."
   ],
   "id": "64a523401bf43e9d"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
